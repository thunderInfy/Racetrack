{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 200\n",
    "COLS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    def widen_hole_transformation(self,racetrack,start_cell,end_cell):\n",
    "        \n",
    "        δ = 1\n",
    "        while(1):\n",
    "            if ((start_cell[1] < δ) or (start_cell[0] < δ)):\n",
    "                racetrack[0:end_cell[0],0:end_cell[1]] = -1\n",
    "                break\n",
    "\n",
    "            if ((end_cell[1]+δ > COLS) or (end_cell[0]+δ > ROWS)):\n",
    "                racetrack[start_cell[0]:ROWS,start_cell[1]:COLS] = -1\n",
    "                break\n",
    "                \n",
    "            δ += 1\n",
    "\n",
    "        return racetrack\n",
    "    \n",
    "    def calculate_valid_fraction(self, racetrack):\n",
    "        '''\n",
    "        Returns the fraction of valid cells in the racetrack\n",
    "        '''\n",
    "        return (len(racetrack[racetrack==0])/(ROWS*COLS))\n",
    "\n",
    "    def mark_finish_states(self, racetrack):\n",
    "        '''\n",
    "        Marks finish states in the racetrack\n",
    "        Returns racetrack\n",
    "        '''\n",
    "        last_col = racetrack[0:ROWS,COLS-1]\n",
    "        last_col[last_col==0] = 2\n",
    "        return racetrack\n",
    "    \n",
    "    def mark_start_states(self, racetrack):\n",
    "        '''\n",
    "        Marks start states in the racetrack\n",
    "        Returns racetrack\n",
    "        '''\n",
    "        last_row = racetrack[ROWS-1,0:COLS]\n",
    "        last_row[last_row==0] = 1\n",
    "        return racetrack\n",
    "    \n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_racetrack(self):\n",
    "        '''\n",
    "        racetrack is a 2d numpy array\n",
    "        codes for racetrack:\n",
    "            0,1,2 : valid racetrack cells\n",
    "            -1: invalid racetrack cell\n",
    "            1: start line cells\n",
    "            2: finish line cells\n",
    "        returns randomly generated racetrack\n",
    "        '''\n",
    "        racetrack = np.zeros((ROWS,COLS),dtype='int')\n",
    "        \n",
    "        frac = 1\n",
    "        while frac > 0.5:    \n",
    "            \n",
    "            #transformation\n",
    "            random_cell = np.random.randint((ROWS,COLS))\n",
    "            random_hole_dims = np.random.randint((ROWS//4,COLS//4))\n",
    "            start_cell = np.array([max(0,x - y//2) for x,y in zip(random_cell,random_hole_dims)])\n",
    "            end_cell = np.array([min(z,x+y) for x,y,z in zip(start_cell,random_hole_dims,[ROWS,COLS])])\n",
    "        \n",
    "            #apply_transformation\n",
    "            racetrack = self.widen_hole_transformation(racetrack, start_cell, end_cell)\n",
    "            frac = self.calculate_valid_fraction(racetrack)\n",
    "        \n",
    "        racetrack = self.mark_start_states(racetrack)\n",
    "        racetrack = self.mark_finish_states(racetrack)\n",
    "        \n",
    "        return racetrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    def get_start_line(self):\n",
    "        '''\n",
    "        Gets start line\n",
    "        '''\n",
    "        self.start_line = np.array([np.array([ROWS-1,j]) for j in range(COLS) if self.racetrack[ROWS-1,j] == 1])\n",
    "        \n",
    "    def get_finish_line(self):\n",
    "        '''\n",
    "        Gets finish line\n",
    "        '''\n",
    "        self.finish_line = np.array([np.array([i,COLS-1]) for i in range(ROWS) if self.racetrack[i,COLS-1] == 2])\n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            racetrack: 2 dimensional numpy array\n",
    "            Q(s,a): 5 dimensional numpy array\n",
    "            C(s,a): 5 dimensional numpy array\n",
    "            π: target policy\n",
    "            start_line: start_line is the set of start states\n",
    "            finish_line: finish_line is the set of finish states\n",
    "            hyperparameters like ε\n",
    "            episode to be an empty list\n",
    "        '''\n",
    "        self.load_racetrack()\n",
    "        self.get_start_line()\n",
    "        self.get_finish_line()\n",
    "        self.load_Q_vals()\n",
    "        self.load_C_vals()\n",
    "        self.load_π()\n",
    "        self.load_rewards()\n",
    "        self.ε = 0.1\n",
    "        self.γ = 1\n",
    "        self.episode = dict({'S':[],'A':[],'probs':[],'R':[None]})\n",
    "        \n",
    "    def save_rewards(self,filename = 'rewards'):\n",
    "        '''\n",
    "        saves self.rewards in rewards.npy file\n",
    "        '''\n",
    "        self.rewards = np.array(self.rewards)\n",
    "        np.save(filename,self.rewards)\n",
    "        self.rewards = list(self.rewards)\n",
    "        \n",
    "    def load_rewards(self):\n",
    "        '''\n",
    "        loads rewards from rewards.npy file\n",
    "        '''\n",
    "        self.rewards = list(np.load('rewards.npy'))\n",
    "        \n",
    "    def save_π(self,filename = 'π.npy'):\n",
    "        '''\n",
    "        saves self.π in π.npy file\n",
    "        '''\n",
    "        np.save(filename,self.π)\n",
    "        \n",
    "    def load_π(self):\n",
    "        '''\n",
    "        loads π from π.npy file\n",
    "        '''\n",
    "        self.π = np.load('π.npy')\n",
    "        \n",
    "    def save_C_vals(self,filename = 'C_vals.npy'):\n",
    "        '''\n",
    "        saves self.C_vals in C_vals.npy file\n",
    "        '''\n",
    "        np.save(filename,self.C_vals)\n",
    "        \n",
    "    def load_C_vals(self):\n",
    "        '''\n",
    "        loads C_vals from C_vals.npy file\n",
    "        '''\n",
    "        self.C_vals = np.load('C_vals.npy')\n",
    "        \n",
    "    def save_Q_vals(self,filename = 'Q_vals.npy'):\n",
    "        '''\n",
    "        saves self.Q_vals in Q_vals.npy file\n",
    "        '''\n",
    "        np.save(filename,self.Q_vals)\n",
    "        \n",
    "    def load_Q_vals(self):\n",
    "        '''\n",
    "        loads Q_vals from Q_vals.npy file\n",
    "        '''\n",
    "        self.Q_vals = np.load('Q_vals.npy')\n",
    "        \n",
    "    def save_racetrack(self,filename = 'racetrack.npy'):\n",
    "        '''\n",
    "        saves self.racetrack in racetrack.npy file\n",
    "        '''\n",
    "        np.save(filename,self.racetrack)\n",
    "        \n",
    "    def load_racetrack(self):\n",
    "        '''\n",
    "        loads racetrack from racetrack.npy file\n",
    "        '''\n",
    "        self.racetrack = np.load('racetrack.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    \n",
    "    def get_new_state(self, state, action):\n",
    "        '''\n",
    "        Get new state after applying action on this state\n",
    "        Assumption: The car keeps on moving with the current velocity and then action is applied to \n",
    "        change the velocity\n",
    "        '''\n",
    "        new_state = state.copy()\n",
    "        new_state[0] = state[0] - state[2]\n",
    "        new_state[1] = state[1] + state[3]\n",
    "        new_state[2] = state[2] + action[0]\n",
    "        new_state[3] = state[3] + action[1]\n",
    "        return new_state\n",
    "    \n",
    "    def select_randomly(self,NUMPY_ARR):\n",
    "        '''\n",
    "        Returns a value uniform randomly from NUMPY_ARR\n",
    "        Here NUMPY_ARR should be 1 dimensional\n",
    "        '''\n",
    "        return np.random.choice(NUMPY_ARR)\n",
    "    \n",
    "    def set_zero(NUMPY_ARR):\n",
    "        '''\n",
    "        Returns NUMPY_ARR after making zero all the elements in it\n",
    "        '''\n",
    "        NUMPY_ARR[:] = 0\n",
    "        return NUMPY_ARR\n",
    "    \n",
    "    def is_finish_line_crossed(self, state, action):\n",
    "        '''\n",
    "        Returns True if the car crosses the finish line\n",
    "                False otherwise\n",
    "        '''\n",
    "        new_state = self.get_new_state(state, action)\n",
    "        old_cell, new_cell = state[0:2], new_state[0:2]\n",
    "        \n",
    "        '''\n",
    "        new_cell's row index will be less\n",
    "        '''\n",
    "        rows = np.array(range(new_cell[0],old_cell[0]+1))\n",
    "        cols = np.array(range(old_cell[1],new_cell[1]+1))\n",
    "        fin = set([tuple(x) for x in self.data.finish_line])\n",
    "        row_col_matrix = [(x,y) for x in rows for y in cols]\n",
    "        intersect = [x for x in row_col_matrix if x in fin]\n",
    "        \n",
    "        return len(intersect) > 0\n",
    "    \n",
    "    def is_out_of_track(self, state, action):\n",
    "        '''\n",
    "        Returns True if the car goes out of track if action is taken on state\n",
    "                False otherwise\n",
    "        '''\n",
    "        new_state = self.get_new_state(state, action)\n",
    "        old_cell, new_cell = state[0:2], new_state[0:2]\n",
    "        \n",
    "        if new_cell[0] < 0 or new_cell[0] >= ROWS or new_cell[1] < 0 or new_cell[1] >= COLS:\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return self.data.racetrack[tuple(new_cell)] == -1\n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self, data, gen):\n",
    "        '''\n",
    "        initialize step_count to be 0\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.gen = gen\n",
    "        self.step_count = 0\n",
    "    \n",
    "    #MEMBER FUNCTIONS\n",
    "    \n",
    "    def reset(self):\n",
    "        self.data.episode = dict({'S':[],'A':[],'probs':[],'R':[None]})\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def start(self):\n",
    "        '''\n",
    "        Makes the velocity of the car to be zero\n",
    "        Returns the randomly selected start state.\n",
    "        '''\n",
    "        state = np.zeros(4,dtype='int')\n",
    "        state[0] = ROWS-1\n",
    "        state[1] = self.select_randomly(self.data.start_line[:,1])\n",
    "        '''\n",
    "        state[2] and state[3] are already zero\n",
    "        '''\n",
    "        return state\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        '''\n",
    "        Returns the reward and new state when action is taken on state\n",
    "        Checks the following 2 cases maintaining the order:\n",
    "            1. car finishes race by crossing the finish line\n",
    "            2. car goes out of track\n",
    "        Ends the episode by returning reward as None and state as usual (which will be terminating)\n",
    "        '''\n",
    "        self.data.episode['A'].append(action)\n",
    "        reward = -1\n",
    "        \n",
    "        if (self.is_finish_line_crossed(state, action)):\n",
    "            new_state = self.get_new_state(state, action)\n",
    "            \n",
    "            self.data.episode['R'].append(reward)\n",
    "            self.data.episode['S'].append(new_state)\n",
    "            self.step_count += 1\n",
    "            \n",
    "            return None, new_state\n",
    "            \n",
    "        elif (self.is_out_of_track(state, action)):\n",
    "            new_state = self.start()\n",
    "        else:\n",
    "            new_state = self.get_new_state(state, action)\n",
    "        \n",
    "        self.data.episode['R'].append(reward)\n",
    "        self.data.episode['S'].append(new_state)\n",
    "        self.step_count += 1\n",
    "        \n",
    "        return reward, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    def possible_actions(self, velocity):\n",
    "        '''\n",
    "        *** Performs two tasks, can be split up ***\n",
    "        Universe of actions:  α = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "                            \n",
    "        Uses constraints to filter out invalid actions given the velocity\n",
    "        \n",
    "        0 <= v_x < 5\n",
    "        0 <= v_y < 5\n",
    "        v_x and v_y cannot be made both zero (you can't take an action which would make them zero simultaneously)\n",
    "        Returns list of possible actions given the velocity\n",
    "        '''\n",
    "        α = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        α = [np.array(x) for x in α]\n",
    "\n",
    "        β = []\n",
    "        for i,x in zip(range(9),α):\n",
    "            new_vel = np.add(velocity,x)\n",
    "            if (new_vel[0] < 5) and (new_vel[0] >= 0) and (new_vel[1] < 5) and (new_vel[1] >= 0) and ~(new_vel[0] == 0 and new_vel[1] == 0):\n",
    "                β.append(i)\n",
    "        β = np.array(β)\n",
    "        \n",
    "        return β\n",
    "    \n",
    "    def map_to_1D(self,action):\n",
    "        α = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        for i,x in zip(range(9),α):\n",
    "            if action[0]==x[0] and action[1]==x[1]:\n",
    "                return i\n",
    "    \n",
    "    def map_to_2D(self,action):\n",
    "        α = [(-1,-1),(-1,0),(0,-1),(-1,1),(0,0),(1,-1),(0,1),(1,0),(1,1)]\n",
    "        return α[action]\n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, state, policy):\n",
    "        '''\n",
    "        Returns action given state using policy\n",
    "        '''\n",
    "        return self.map_to_2D(policy(state, self.possible_actions(state[2:4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monte_Carlo_Control:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    \n",
    "    def evaluate_target_policy(self):\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "        self.data.episode['S'].append(state)\n",
    "        rew = -1\n",
    "        while rew!=None:\n",
    "            action = agent.get_action(state,self.generate_target_policy_action)\n",
    "            rew, state = env.step(state,action)\n",
    "            \n",
    "        self.data.rewards.append(sum(self.data.episode['R'][1:]))\n",
    "        \n",
    "    \n",
    "    def plot_rewards(self):\n",
    "        ax, fig = plt.subplots(figsize=(30,15))\n",
    "        x = np.arange(1,len(self.data.rewards)+1)\n",
    "        plt.plot(x*10, self.data.rewards, linewidth=0.5, color = '#BB8FCE')\n",
    "        plt.xlabel('Episode number', size = 20)\n",
    "        plt.ylabel('Reward',size = 20)\n",
    "        plt.title('Plot of Reward vs Episode Number',size=20)\n",
    "        plt.xticks(size=20)\n",
    "        plt.yticks(size=20)\n",
    "        plt.savefig('RewardGraph.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def save_your_work(self):\n",
    "        self.data.save_Q_vals()\n",
    "        self.data.save_C_vals()\n",
    "        self.data.save_π()\n",
    "        self.data.save_rewards()\n",
    "    \n",
    "    def determine_probability_behaviour(self, state, action, possible_actions):\n",
    "        best_action = self.data.π[tuple(state)]\n",
    "        num_actions = len(possible_actions)\n",
    "        \n",
    "        if best_action in possible_actions:\n",
    "            if action == best_action:\n",
    "                prob = 1 - self.data.ε + self.data.ε/num_actions\n",
    "            else:\n",
    "                prob = self.data.ε/num_actions\n",
    "        else:\n",
    "            prob = 1/num_actions\n",
    "        \n",
    "        self.data.episode['probs'].append(prob)\n",
    "    \n",
    "    def generate_target_policy_action(self, state, possible_actions):\n",
    "        '''\n",
    "        Returns target policy action, takes state and\n",
    "        returns an action using this policy\n",
    "        '''\n",
    "        if self.data.π[tuple(state)] in possible_actions:\n",
    "            action = self.data.π[tuple(state)]\n",
    "        else:\n",
    "            action = np.random.choice(possible_actions)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def generate_behavioural_policy_action(self, state, possible_actions):\n",
    "        '''\n",
    "        Returns behavioural policy action\n",
    "        which would be ε-greedy π policy, takes state and\n",
    "        returns an action using this ε-greedy π policy\n",
    "        '''\n",
    "        if np.random.rand() > self.data.ε and self.data.π[tuple(state)] in possible_actions:\n",
    "            action = self.data.π[tuple(state)]\n",
    "        else:\n",
    "            action = np.random.choice(possible_actions)\n",
    "        \n",
    "        self.determine_probability_behaviour(state, action, possible_actions)\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        Initialize, for all s ∈ S, a ∈ A(s):\n",
    "            data.Q(s, a) ← arbitrary (done in Data)\n",
    "            data.C(s, a) ← 0 (done in Data)\n",
    "            π(s) ← argmax_a Q(s,a) \n",
    "            (with ties broken consistently) \n",
    "            (some consistent approach needs to be followed))\n",
    "        '''\n",
    "        self.data = data\n",
    "        for i in range(ROWS):\n",
    "            for j in range(COLS):\n",
    "                if self.data.racetrack[i,j]!=-1:\n",
    "                    for k in range(5):\n",
    "                        for l in range(5):\n",
    "                            self.data.π[i,j,k,l] = np.argmax(self.data.Q_vals[i,j,k,l])\n",
    "    \n",
    "    def control(self,env,agent):\n",
    "        '''\n",
    "        Performs MC control using episode list [ S0 , A0 , R1, . . . , ST −1 , AT −1, RT , ST ]\n",
    "        G ← 0\n",
    "        W ← 1\n",
    "        For t = T − 1, T − 2, . . . down to 0:\n",
    "            G ← γ*G + R_t+1\n",
    "            C(St, At ) ← C(St,At ) + W\n",
    "            Q(St, At ) ← Q(St,At) + (W/C(St,At))*[G − Q(St,At )]\n",
    "            π(St) ← argmax_a Q(St,a) (with ties broken consistently)\n",
    "            If At != π(St) then exit For loop\n",
    "            W ← W * (1/b(At|St))        \n",
    "        '''\n",
    "        env.reset()\n",
    "        state = env.start()\n",
    "        self.data.episode['S'].append(state)\n",
    "        rew = -1\n",
    "        while rew!=None:\n",
    "            action = agent.get_action(state,self.generate_behavioural_policy_action)\n",
    "            rew, state = env.step(state,action)\n",
    "        \n",
    "        G = 0\n",
    "        W = 1\n",
    "        T = env.step_count\n",
    "    \n",
    "        for t in range(T-1,-1,-1):\n",
    "            G = data.γ * G + self.data.episode['R'][t+1]\n",
    "            S_t = tuple(self.data.episode['S'][t])\n",
    "            A_t = agent.map_to_1D(self.data.episode['A'][t])\n",
    "            \n",
    "            S_list = list(S_t)\n",
    "            S_list.append(A_t)\n",
    "            SA = tuple(S_list)\n",
    "            \n",
    "            self.data.C_vals[SA] += W\n",
    "            self.data.Q_vals[SA] += (W*(G-self.data.Q_vals[SA]))/(self.data.C_vals[SA])           \n",
    "            self.data.π[S_t] = np.argmax(self.data.Q_vals[S_t])\n",
    "            if A_t!=self.data.π[S_t]:\n",
    "                break\n",
    "            W /= self.data.episode['probs'][t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    \n",
    "    #HELPFUL FUNCTIONS\n",
    "    \n",
    "    def visualize_episode():\n",
    "        for i in range(self.data.episode['S']):\n",
    "            vis.visualize_racetrack(i)\n",
    "    \n",
    "    def create_window(self):\n",
    "        '''\n",
    "        Creates window and assigns self.display variable\n",
    "        '''\n",
    "        self.display = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption(\"Racetrack\")\n",
    "    \n",
    "    def setup(self):\n",
    "        '''\n",
    "        Does things which occur only at the beginning\n",
    "        '''\n",
    "        self.cell_edge = 5\n",
    "        self.width = COLS*self.cell_edge\n",
    "        self.height = ROWS*self.cell_edge\n",
    "        self.create_window()\n",
    "        self.window = True\n",
    "\n",
    "    def close_window(self):\n",
    "        self.window = False\n",
    "        pygame.quit()\n",
    "\n",
    "    def draw(self, state = np.array([])):\n",
    "        self.display.fill(0)\n",
    "        for i in range(ROWS):\n",
    "            for j in range(COLS):\n",
    "                if self.data.racetrack[i,j]!=-1:\n",
    "                    if self.data.racetrack[i,j] == 0:\n",
    "                        color = (255,0,0)\n",
    "                    elif self.data.racetrack[i,j] == 1:\n",
    "                        color = (255,255,0)\n",
    "                    elif self.data.racetrack[i,j] == 2:\n",
    "                        color = (0,255,0)\n",
    "                    pygame.draw.rect(self.display,color,((j*self.cell_edge,i*self.cell_edge),(self.cell_edge,self.cell_edge)),1)\n",
    "        \n",
    "        if len(state)>0:\n",
    "            pygame.draw.rect(self.display,(255,255,255),((state[1]*self.cell_edge,state[0]*self.cell_edge),(self.cell_edge,self.cell_edge)),0)\n",
    "        \n",
    "        pygame.display.update()\n",
    "        \n",
    "        global count\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.loop = False\n",
    "                self.close_window()\n",
    "                return 'stop'\n",
    "            elif event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE:\n",
    "                pygame.image.save(vis.display, str(count)+'.png')\n",
    "                count += 1\n",
    "                self.loop = False\n",
    "                \n",
    "        return None\n",
    "        \n",
    "    def visualize_racetrack(self, state = np.array([])):\n",
    "        '''\n",
    "        Draws Racetrack in a pygame window\n",
    "        '''\n",
    "        if self.window == False:\n",
    "            self.setup()\n",
    "        self.loop = True\n",
    "        while(self.loop):\n",
    "            ret = self.draw(state)\n",
    "            if ret!=None:\n",
    "                return ret\n",
    "    \n",
    "    #CONSTRUCTOR\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.window = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = Data()\n",
    "gen = Generator()\n",
    "env = Environment(data,gen)\n",
    "mcc = Monte_Carlo_Control(data)\n",
    "vis = Visualizer(data)\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below code blocks suitably to carry out tasks you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.visualize_racetrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(50000):\n",
    "#     mcc.control(env,agent)\n",
    "    \n",
    "#     if i%10 == 9:\n",
    "#         mcc.evaluate_target_policy()\n",
    "    \n",
    "#     if i%100 == 99:\n",
    "#         mcc.save_your_work()\n",
    "#         mcc.plot_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch = 50\n",
    "# S = sum(data.rewards[:ch])/ch\n",
    "\n",
    "# R = []\n",
    "\n",
    "# for i in range(ch,len(data.rewards)):\n",
    "#     R.append(S)\n",
    "#     S *= ch\n",
    "#     S += data.rewards[i]\n",
    "#     S -= data.rewards[i-ch]\n",
    "#     S /= ch\n",
    "\n",
    "# ax, fig = plt.subplots(figsize=(60,30))\n",
    "# x = np.arange(1,len(R)+1)\n",
    "# plt.plot(x*10, R, linewidth=1, color = '#BB8FCE')\n",
    "# plt.xlabel('Episode number', size = 40)\n",
    "# plt.ylabel('Reward',size = 40)\n",
    "# plt.title('Plot of Reward vs Episode Number',size=40)\n",
    "# plt.xticks(size=40)\n",
    "# plt.yticks(size=40)\n",
    "# plt.savefig('RewardGraph2.png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = Generator()\n",
    "\n",
    "# data.racetrack = gen.generate_racetrack()\n",
    "\n",
    "# vis.visualize_racetrack()\n",
    "\n",
    "# data.save_racetrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.Q_vals = np.random.rand(ROWS,COLS,5,5,9)*400 - 500\n",
    "\n",
    "# data.rewards = []\n",
    "\n",
    "# data.C_vals = np.zeros((ROWS,COLS,5,5,9))\n",
    "\n",
    "# data.π = np.zeros((ROWS,COLS,5,5),dtype='int')\n",
    "\n",
    "\n",
    "# data.save_Q_vals()\n",
    "# data.save_C_vals()\n",
    "# data.save_rewards()\n",
    "# data.save_π()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# state = env.start()\n",
    "# mcc.data.episode['S'].append(state)\n",
    "# rew = -1\n",
    "# while rew!=None:\n",
    "#     action = agent.get_action(state,mcc.generate_target_policy_action)\n",
    "#     rew, state = env.step(state,action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([199,   4,   0,   0]),\n",
       " array([199,   4,   1,   1]),\n",
       " array([198,   5,   2,   0]),\n",
       " array([196,   5,   3,   1]),\n",
       " array([193,   6,   3,   1]),\n",
       " array([190,   7,   4,   1]),\n",
       " array([186,   8,   4,   2]),\n",
       " array([182,  10,   4,   3]),\n",
       " array([178,  13,   4,   3]),\n",
       " array([174,  16,   4,   2]),\n",
       " array([170,  18,   4,   2]),\n",
       " array([166,  20,   4,   2]),\n",
       " array([162,  22,   3,   2]),\n",
       " array([159,  24,   3,   3]),\n",
       " array([156,  27,   4,   2]),\n",
       " array([152,  29,   4,   1]),\n",
       " array([148,  30,   4,   2]),\n",
       " array([144,  32,   4,   3]),\n",
       " array([140,  35,   4,   3]),\n",
       " array([136,  38,   3,   2]),\n",
       " array([133,  40,   4,   3]),\n",
       " array([129,  43,   4,   4]),\n",
       " array([125,  47,   3,   3]),\n",
       " array([122,  50,   3,   3]),\n",
       " array([119,  53,   4,   4]),\n",
       " array([115,  57,   4,   4]),\n",
       " array([111,  61,   4,   4]),\n",
       " array([107,  65,   3,   3]),\n",
       " array([104,  68,   4,   4]),\n",
       " array([100,  72,   4,   4]),\n",
       " array([96, 76,  4,  4]),\n",
       " array([92, 80,  4,  4]),\n",
       " array([88, 84,  4,  4]),\n",
       " array([84, 88,  4,  3]),\n",
       " array([80, 91,  4,  4]),\n",
       " array([76, 95,  3,  4]),\n",
       " array([73, 99,  2,  3])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.episode['S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in data.episode['S']:\n",
    "#     if vis.visualize_racetrack(i) == 'stop':\n",
    "#         break\n",
    "# vis.close_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imageio\n",
    "# filenames = []\n",
    "\n",
    "# for i in range(37):\n",
    "#     filenames.append(str(i)+'.png')\n",
    "\n",
    "# images = []\n",
    "# for filename in filenames:\n",
    "#     images.append(imageio.imread(filename))\n",
    "# imageio.mimsave('movie.gif', images, duration = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
